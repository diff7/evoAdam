{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В этом ноутбуке будет описание: \n",
    "1. Результатов\n",
    "2. Параметров обучения\n",
    "3. Частичные свертки - описание (одна из основных идей статьи)\n",
    "4. Описание лосс функций и их графики\n",
    "\n",
    "\n",
    "Основая статья: https://arxiv.org/pdf/1804.07723.pdf <br>\n",
    "Реопозиторий имплементации: https://github.com/naoto0804/pytorch-inpainting-with-partial-conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сгенерируем результат обучения\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import opt\n",
    "from net import PConvUNet\n",
    "from util.io import load_ckpt\n",
    "from util.image import unnormalize\n",
    "from evaluation import evaluate\n",
    "from my_dataset import MyDataset\n",
    "\n",
    "\n",
    "root='./my_train'\n",
    "mask_root='./masks'\n",
    "save_dir='./snapshots/'\n",
    "image_size=256\n",
    "wieghts = './snapshots/default/ckpt/1010100.pth'\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "size = ( image_size,  image_size)\n",
    "img_tf = transforms.Compose(\n",
    "    [transforms.Resize(size=size), transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=opt.MEAN, std=opt.STD)])\n",
    "mask_tf = transforms.Compose(\n",
    "    [transforms.Resize(size=size), transforms.ToTensor()])\n",
    "\n",
    "dataset_val = MyDataset(root,  mask_root, img_tf, mask_tf, 'val')\n",
    "\n",
    "model = PConvUNet().to(device)\n",
    "load_ckpt(wieghts, [('model', model)])\n",
    "\n",
    "model.eval()\n",
    "evaluate(model, dataset_val, device, './results/result3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результат обучения:\n",
    "\n",
    "1. Изображение с маской\n",
    "2. Маска\n",
    "3. Результат восстановления \n",
    "3. Результат восстановления с маской\n",
    "4. Оригинал\n",
    "\n",
    "\n",
    "### Пример 1\n",
    "<img src='./results/result.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Пример 2\n",
    "<img src='./results/result2.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Пример 3\n",
    "<img src='./results/result3.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параметры обучения\n",
    "\n",
    "Обучение просиходило на ноутбуке со слабым GPU, в связи с этим модель дообучалась на весах модели обученной на датасете Places 3 эпохи и 3 эпохи файнтьюнинг с замороженными весами.\n",
    "\n",
    "lr = 2e-4\n",
    "lr finetune = 5e-5\n",
    "\n",
    "Размер батча 2\n",
    "\n",
    "Графики лосс функций считались только на тренировочном датасете с помощью тензор борда.\n",
    "\n",
    "Число изображений в тренировочном датасете составляет 4000 (чуть меньше после удаления поврежденных изображений).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Частичные свертки\n",
    "\n",
    "<img src=\"./results/report_images/partial_conv.png\">\n",
    "\n",
    "\n",
    "W - матрица весов <br>\n",
    "X - значения пикселей свертки ( входного изображения для первого слоя )  <br>\n",
    "M - бинарная маска <br>\n",
    "\n",
    "Таким образом, в предсказании учавствуют только те пиксели которые не были занулены бинарной маской. <br> <br>\n",
    "Множитель sum(1)/sum(M), здесь 1 матрица такого же размера как и M, отвечает за скейлинг. Т.е. если в текущей свертке болше половина нулей то sum(1)/sum(M) = 2. Соответсвенно чем меньше нулей тем меньше коэффициент, но не меньше 1.  <br>\n",
    "<br>\n",
    "*<i>Мое личное мнение что должно работать не хуже и без sum(1)/sum(M), но нужно почитать ссылки на другие работы где используют тот же механизм и попробовать без них. Так же где то встречал упоминание что из за этого множителя больший вес получают пиксели по краям маски, но механизм мне пока не понятен.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./results/report_images/partial_conv_masks.png\">\n",
    "\n",
    "После операции частичной свертки обновляется маска. Если было предсказано хотя бы одно правильное значение пикселя. Таким образом мы постепнно стягиваем бианарную маску что отношение sum(1)/sum(M) становится равным 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лосс функции\n",
    "<img src=\"./results/report_images/losses12.png\">\n",
    "\n",
    "M - маска <br>\n",
    "Iout - предсказанное изображение <br>\n",
    "Igt - ground truth <br>\n",
    "Nigt = C x H x W - число элементов на входе ( 1/N просто нормализуем значение лосса, что бычоно было инвариантным к изображениям разных размеров )<br>\n",
    "<br>\n",
    "Tаким образом Loss hole отвечает за ошибку восстановления частей изображения с маской. <br>\n",
    "А Loss Valid за ошибку восстановления изображения без маски. <br>\n",
    "\n",
    "Кажется что это основные лоссы в работе, но авторы так же покаызвают важность других лоссов в основнм perceptual и style loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./results/report_images/loss3.png\">\n",
    "\n",
    "\n",
    "\n",
    "Perceptual Loss, здесь считается L1 для входного и выходного изображенний после их проекции в пространстве ImageNet-pretrained VGG-16, Ψ - здесь проекции соответствующих изображений в VGG-16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./results/style.png\">\n",
    "\n",
    "Style Loss - делает примерно тоже самое что и perceptual loss, но сначало применяется определитель Грама. <br>\n",
    "*<i>Если не ошибаюсь то с помощью определителся Грамма мы находим оптимальную ортогональную проекцию. (могу понять это интуитивно, но до конца не понимаю механику)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss hole\n",
    "<img src=\"./results/loss_hole.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss valid\n",
    "<img src=\"./results/loss_valid.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss perceptual\n",
    "<img src=\"./results/loss_prc.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "1. Первый плохой момент это то что во время обучения не считался никакой скор (кроме лоссов) на валидации и он не сравнивался с трейном.\n",
    "2. Второй плохой момент, по полученным картинкам кажется что сетка переобучилась, так как в некоторых местах она очень хорошо восстанавливает изображнеия, но с другой стороны данные были разбиты на трейн, тест и валидацию так что этого не должно было быть.К тому же и число итераций было достаточно малым. Возможно и то что наши объекты могут быть очень однотипными, поэтому они достаточно хорошо восстанавливаются. Сравнение метрик на валидации и на трейне могло бы добавить ясности. Может быть и то что предобученная на датасете Places сеть более или менее хорошо справляется с нашей задачей и можно было вообще не обучать.\n",
    "\n",
    "3. Loss holes и Loss perceptual измерялись каждый 100 итераций с батчом 2, поэтому тут весьма ожидаемы большие колебания. Для более равномерного графика нужно больше ресурсов на обучение и возможно больше данных. \n",
    "4. Loss valid (не путать с валидацией) это лосс по части изображения которую не нужно было восстанавливать, те без маски с затертой частью. Это обозначает что модель незначитиельно искажает оригинальное изображение. И при этом за очень малое число итераций модель научилась этого не делать. (что тоже странно и похорошему нужно посмотреть эту часть подробнее)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Что еще можно было бы сделать:\n",
    "\n",
    "1. Посчитать mse мли psnr на тесте и валидационном датасете.\n",
    "2. Посмотреть разбиение классов на нашем выбранном тренировочном датасете. Из всех изображений в датасете human protein я выбрал 4000 для трейна, методом sample в pandas, а это не обещает сохранения пропорции между классами. Скорее всего ее нет.\n",
    "3. Сравнить psnr или mse на разных классах. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
